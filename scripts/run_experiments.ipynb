{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw83KAePAhaS"
      },
      "source": [
        "# The GEESE Challenge: running the experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnwsOpjda_YW"
      },
      "source": [
        "In this notebook we will be going through the steps to perform the three steps involved in the GEESE task using the e-rte-3-it dataset [1] as our data source."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAov81vTbL2K"
      },
      "source": [
        "### Install Miniconda and LM-Eval\n",
        "(Tip: we suggest to creat a [Miniconda](https://docs.anaconda.com/miniconda/) environment and work within in.)\n",
        "\n",
        "First, you don't have it already, install the LM-Eval library by running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hiosGzq_qZg",
        "outputId": "6ab73e5e-1f54-417e-a388-07e0d870b132"
      },
      "outputs": [],
      "source": [
        "%pip install git+https://github.com/EleutherAI/lm-evaluation-harness.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Generate the explanations for the given labels with M1\n",
        "We will use llama-3-3B-instruct as M1 to generate the explanations for the entailment labels in e-rte-3-it.\n",
        "\n",
        "The script is now running on CPU to overcome out-of-memory issues, but can be run on GPU by uncommenting the relevant lines.\n",
        "\n",
        "**IMPORTANT NOTE**\n",
        "\n",
        "This step is **not** implemented within lm_eval and is not included in the gist configuration file. Here we provide custom code to generate explanations with llama-3. Change model, prompt and generation parameters as you wish, and save it into a HF's Datatset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, tqdm, torch, gc\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset, load_from_disk\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"<your_hf_token>\"  # Replace with your HuggingFace token\n",
        "\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Ensure enough GPU memory is available\n",
        "\n",
        "def step_1_generate_explanation(model_name, dataset_name):\n",
        "    # Use the first GPU if available\n",
        "    device = \"cpu\" #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load the model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    #model = model.to(device) # to use GPU\n",
        "\n",
        "    # Create a pipeline for text generation\n",
        "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, pad_token_id=tokenizer.eos_token_id, device=0)\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = load_dataset(dataset_name, split='test')\n",
        "    model_nick = \"llama3\"\n",
        "    \n",
        "    # Function to generate explanations\n",
        "    def add_generated_explanation(example):\n",
        "        text_t = example['text_t']\n",
        "        text_h = example['text_h']\n",
        "        label_str = example['label']\n",
        "        \n",
        "        def generate_explanation(sentence_1, sentence_2, label, exp_type=\"Explain why.\"):\n",
        "            example_sentences = f\"Sentence 1: {sentence_1.strip()}\\nSentence 2: {sentence_2.strip()}\"\n",
        "            prompt = f\"Your task is to provide an explanation for the label assigned for the entailment relationship between two sentences.\\n{example_sentences}\\nEntailment label: {label}\\n{exp_type}\"\n",
        "\n",
        "            #with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "            explanation = generator(prompt, max_length=410, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95, temperature=0.7)\n",
        "\n",
        "            return explanation[0]['generated_text'].split(f'{exp_type}')[-1]\n",
        "\n",
        "        explanation_model = generate_explanation(sentence_1=text_t, sentence_2=text_h, label=label_str)\n",
        "        example[f'anon_{model_nick}'] = explanation_model.strip()\n",
        "\n",
        "        return example\n",
        "    \n",
        "    # Shuffle (and optionally select a smaller chunk of the dataset)\n",
        "    dataset = dataset.shuffle(123) # dataset.shuffle(123).select(range(20))\n",
        "\n",
        "    # Process the dataset (not in batches for memory control)\n",
        "    updated_dataset = dataset.map(add_generated_explanation, batched=False)\n",
        "\n",
        "    output_dir = f\"./explained_{model_nick}\"\n",
        "    updated_dataset.save_to_disk(output_dir)\n",
        "\n",
        "    print(f\"Generation completed. Dataset saved to {output_dir}\")\n",
        "\n",
        "def main():\n",
        "    step_1_generate_explanation(model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\", dataset_name=\"azaninello/e-RTE-3-it\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Post-process explanations\n",
        "After saving your explanations into a dataset, they must be post-processed to prevent label leakage. This step is mandatory. \n",
        "\n",
        "Save the anonimized dataset with you new explanations and push it to the HF hub.\n",
        "\n",
        "(Tip: label your anonymized explanations with a memorable name: you will use them in the next evaluation step. )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: The entailment label is UNKNOWN because the two sentences do not have a direct entailment relationship. Sentence 1 states that it is rumored that Metin Kaplan ordered the murder of Ibrahim Sofu, while Sentence 2 states that Ibrahim Sofu was killed by Metin Kaplan. The two sentences present different information, with Sentence 1 providing a rumor or unverified information, and Sentence 2 presenting a factual statement. There is no logical connection between the two sentences that would imply one sentence is a direct consequence of the other. Therefore, the entailment label is UNKNOWN, indicating that the relationship between the two sentences is not clear or cannot be determined. \n",
            "\n",
            "Please note that this explanation is based on the assumption that the entailment label is assigned based on the logical relationship between the two sentences. If the entailment label is assigned based on other criteria, such as the similarity or overlap between the two sentences, a different explanation may be necessary.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 46.99ba/s]\n",
            "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  3.29it/s]\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Post-processed: The XXX label is XXX because the two sentences do not have a direct XXX relationship. Sentence 1 states that it is rumored that Metin Kaplan ordered the murder of Ibrahim Sofu, while Sentence 2 states that Ibrahim Sofu was killed by Metin Kaplan. The two sentences present different information, with Sentence 1 providing a rumor or unverified information, and Sentence 2 presenting a factual statement. There is XXX logical connection between the two sentences that would imply one sentence is a direct consequence of the other. Therefore, the XXX label is XXX, indicating that the relationship between the two sentences is not clear or cannot be determined. \n",
            "\n",
            "Please note that this explanation is based on the assumption that the XXX label is assigned based on the logical relationship between the two sentences. If the XXX label is assigned based on other criteria, such as the similarity or overlap between the two sentences, a different explanation may be necessary.\n",
            "Upload completed. Explanations saved as geese-llama-3\n"
          ]
        }
      ],
      "source": [
        "# Check the first example of the locally saved dataset\n",
        "# *** IMPORTANT STEP *** Then, anonymize the explanations to prevent label-leakage \n",
        "# Finally, save the new dataset to the Hugging Face Hub\n",
        "# Check the anoinimized example\n",
        "\n",
        "import re\n",
        "from datasets import load_from_disk\n",
        "\n",
        "model_nick = \"llama3\"\n",
        "output_dir = f\"./explained_{model_nick}\"\n",
        "\n",
        "# Load the saved dataset\n",
        "loaded_dataset = load_from_disk(output_dir)\n",
        "\n",
        "print(\"Original:\", loaded_dataset[0][f'anon_{model_nick}'])  # Display the first example, including the generated/anonimized explanations\n",
        "\n",
        "anon_pattern = r\"(\\bYES\\b|\\bNO\\b|\\bUNKNOWN\\b|\\bentail\\w*\\b|\\bcontradict\\w*\\b|\\bneutral\\w*\\b)\"\n",
        "subst_str = \"XXX\"\n",
        "    \n",
        "def anonymize_explanations(example):\n",
        "            # Add the generated explanation to the original model\n",
        "            example['anon_human'] = re.sub(anon_pattern, subst_str, example['explanation'].strip(), flags=re.IGNORECASE)\n",
        "            example[f'anon_{model_nick}'] = re.sub(anon_pattern, subst_str, example[f'anon_{model_nick}'].strip(), flags=re.IGNORECASE)\n",
        "            return example\n",
        "\n",
        "anon_dataset = loaded_dataset.map(anonymize_explanations, batched=False)\n",
        "\n",
        "public_name = \"geese-llama-3\"\n",
        "anon_dataset.push_to_hub(public_name, private=False)\n",
        "\n",
        "print(\"Post-processed:\", anon_dataset[0][f'anon_{model_nick}'])\n",
        "\n",
        "print(f\"Upload completed. Explanations saved as {public_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example of anonimized explanation: The XXX label is XXX because the two sentences do not have a direct XXX relationship. Sentence 1 states that it is rumored that Metin Kaplan ordered the murder of Ibrahim Sofu, while Sentence 2 states that Ibrahim Sofu was killed by Metin Kaplan. The two sentences present different information, with Sentence 1 providing a rumor or unverified information, and Sentence 2 presenting a factual statement. There is XXX logical connection between the two sentences that would imply one sentence is a direct consequence of the other. Therefore, the XXX label is XXX, indicating that the relationship between the two sentences is not clear or cannot be determined. \n",
            "\n",
            "Please note that this explanation is based on the assumption that the XXX label is assigned based on the logical relationship between the two sentences. If the XXX label is assigned based on other criteria, such as the similarity or overlap between the two sentences, a different explanation may be necessary.\n"
          ]
        }
      ],
      "source": [
        "print(\"Example of anonimized explanation:\", anon_dataset[0][f'anon_{model_nick}'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rfUeX6n_wkK"
      },
      "source": [
        "### Step 2: Predict labels with explanations with M2\n",
        "\n",
        "We configure the prediction tasks of the M2 model as LM-EVAL tasks with YAML configs. With configs, you can fill preset fields to easily set up a task. Here, we write 4 different YAML config files under the *geese_tasks* tag for multiple-choice evaluation on the e-rte-3-it labels (read from our newly generated dataset). \n",
        "\n",
        "Here we define four configurations corresponding to four explanation sets given as \"Hint\": \n",
        "1. no explanation (Hint = \"Not given.\")\n",
        "2. dummy explanation (Hint = text_h)\n",
        "3. the anonimized human explanations (Hint = anon_human)\n",
        "4. the explanations generated in Step 1 (in our case Hint = anon_llama3)\n",
        "\n",
        "For evaluation, you connot change the prompt and generation parameters. In the gist configuration file presented at the CALAMITA Challenge, we currently report the **geese_human** configuration, but you are encouraged to propose your own explanations **after post-processing** (see above) before using them, and to read the anonimized explanations from the dataset produced in Step 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fthNg3ywO-kA"
      },
      "outputs": [],
      "source": [
        "YAML_geese_noexp_string = '''\n",
        "tag: geese_tasks\n",
        "task: geese_noexp\n",
        "dataset_path: azaninello/geese-llama-3\n",
        "dataset_name: default\n",
        "output_type: multiple_choice\n",
        "test_split: test\n",
        "validation_split: test\n",
        "doc_to_text: \"Sentence 1:{{text_t}}Sentence 2: {{text_h}}Hint: Not given.\\nEntailment label:\"\n",
        "doc_to_target: label\n",
        "doc_to_choice: [\"YES\", \"NO\", \"UNKNOWN\"]\n",
        "metric_list:\n",
        "  - metric: acc\n",
        "    aggregation: mean\n",
        "    higher_is_better: true\n",
        "'''\n",
        "with open('no-exp.yaml', 'w') as f:\n",
        "    f.write(YAML_geese_noexp_string)\n",
        "\n",
        "YAML_geese_dummy_string = '''\n",
        "tag: geese_tasks\n",
        "task: geese_dummy\n",
        "dataset_path: azaninello/geese-llama-3\n",
        "dataset_name: default\n",
        "output_type: multiple_choice\n",
        "test_split: test\n",
        "validation_split: test\n",
        "doc_to_text: \"Sentence 1:{{text_t}}Sentence 2: {{text_h}}Hint: {{text_h}}.\\nEntailment label:\"\n",
        "doc_to_target: label\n",
        "doc_to_choice: [\"YES\", \"NO\", \"UNKNOWN\"]\n",
        "metric_list:\n",
        "  - metric: acc\n",
        "    aggregation: mean\n",
        "    higher_is_better: true\n",
        "'''\n",
        "with open('dummy-exp.yaml', 'w') as f:\n",
        "    f.write(YAML_geese_dummy_string)\n",
        "\n",
        "YAML_geese_human_string = '''\n",
        "tag: geese_tasks\n",
        "task: geese_human\n",
        "dataset_path: azaninello/geese-llama-3\n",
        "dataset_name: default\n",
        "output_type: multiple_choice\n",
        "test_split: test\n",
        "validation_split: test\n",
        "doc_to_text: \"Sentence 1:{{text_t}}Sentence 2: {{text_h}}Hint: {{anon_human}}.\\nEntailment label:\"\n",
        "doc_to_target: label\n",
        "doc_to_choice: [\"YES\", \"NO\", \"UNKNOWN\"]\n",
        "metric_list:\n",
        "  - metric: acc\n",
        "    aggregation: mean\n",
        "    higher_is_better: true\n",
        "'''\n",
        "with open('human-exp.yaml', 'w') as f:\n",
        "    f.write(YAML_geese_human_string)\n",
        "\n",
        "YAML_geese_llama_string = '''\n",
        "tag: geese_tasks\n",
        "task: geese_llama3\n",
        "dataset_path: azaninello/geese-llama-3\n",
        "dataset_name: default\n",
        "output_type: multiple_choice\n",
        "test_split: test\n",
        "validation_split: test\n",
        "doc_to_text: \"Sentence 1:{{text_t}}Sentence 2: {{text_h}}Hint: {{anon_llama3}}.\\nEntailment label:\"\n",
        "doc_to_target: label\n",
        "doc_to_choice: [\"YES\", \"NO\", \"UNKNOWN\"]\n",
        "metric_list:\n",
        "  - metric: acc\n",
        "    aggregation: mean\n",
        "    higher_is_better: true\n",
        "'''\n",
        "with open('llama3-exp.yaml', 'w') as f:\n",
        "    f.write(YAML_geese_llama_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Evaluate explanations on downstream task (M2 performance)\n",
        "\n",
        "Here we provide custom code to test the generated explanations llama-3-8B as M2 on label prediciton. \n",
        "\n",
        "In this step you can can only change the model to use as M2 and recall your own configuration(s) through the task (see above).\n",
        "\n",
        "**IMPORTANT NOTE**\n",
        "\n",
        "This step is **IS** the one implemented within lm_eval for the geese_human configuration described in the gist [configuration file](https://gist.github.com/andreazaninello/6d92daeaa1c264477b32fd22acfbd818). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XceRKCuuDtbn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usage: accelerate <command> [<args>] launch [-h] [--config_file CONFIG_FILE]\n",
            "                                            [--quiet] [--cpu] [--multi_gpu]\n",
            "                                            [--tpu] [--ipex]\n",
            "                                            [--mixed_precision {no,fp16,bf16,fp8}]\n",
            "                                            [--num_processes NUM_PROCESSES]\n",
            "                                            [--num_machines NUM_MACHINES]\n",
            "                                            [--num_cpu_threads_per_process NUM_CPU_THREADS_PER_PROCESS]\n",
            "                                            [--enable_cpu_affinity]\n",
            "                                            [--dynamo_backend {no,eager,aot_eager,inductor,aot_ts_nvfuser,nvprims_nvfuser,cudagraphs,ofi,fx2trt,onnxrt,tensorrt,aot_torchxla_trace_once,torhchxla_trace_once,ipex,tvm}]\n",
            "                                            [--dynamo_mode {default,reduce-overhead,max-autotune}]\n",
            "                                            [--dynamo_use_fullgraph]\n",
            "                                            [--dynamo_use_dynamic]\n",
            "                                            [--use_deepspeed] [--use_fsdp]\n",
            "                                            [--use_megatron_lm] [--use_xpu]\n",
            "                                            [--gpu_ids GPU_IDS]\n",
            "                                            [--same_network]\n",
            "                                            [--machine_rank MACHINE_RANK]\n",
            "                                            [--main_process_ip MAIN_PROCESS_IP]\n",
            "                                            [--main_process_port MAIN_PROCESS_PORT]\n",
            "                                            [-t TEE] [--log_dir LOG_DIR]\n",
            "                                            [--role ROLE]\n",
            "                                            [--rdzv_backend RDZV_BACKEND]\n",
            "                                            [--rdzv_conf RDZV_CONF]\n",
            "                                            [--max_restarts MAX_RESTARTS]\n",
            "                                            [--monitor_interval MONITOR_INTERVAL]\n",
            "                                            [-m] [--no_python] [--tpu_cluster]\n",
            "                                            [--no_tpu_cluster]\n",
            "                                            [--tpu_use_sudo] [--vm VM]\n",
            "                                            [--env ENV]\n",
            "                                            [--main_training_function MAIN_TRAINING_FUNCTION]\n",
            "                                            [--downcast_bf16]\n",
            "                                            [--deepspeed_config_file DEEPSPEED_CONFIG_FILE]\n",
            "                                            [--zero_stage ZERO_STAGE]\n",
            "                                            [--offload_optimizer_device OFFLOAD_OPTIMIZER_DEVICE]\n",
            "                                            [--offload_param_device OFFLOAD_PARAM_DEVICE]\n",
            "                                            [--offload_optimizer_nvme_path OFFLOAD_OPTIMIZER_NVME_PATH]\n",
            "                                            [--offload_param_nvme_path OFFLOAD_PARAM_NVME_PATH]\n",
            "                                            [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
            "                                            [--gradient_clipping GRADIENT_CLIPPING]\n",
            "                                            [--zero3_init_flag ZERO3_INIT_FLAG]\n",
            "                                            [--zero3_save_16bit_model ZERO3_SAVE_16BIT_MODEL]\n",
            "                                            [--deepspeed_hostfile DEEPSPEED_HOSTFILE]\n",
            "                                            [--deepspeed_exclusion_filter DEEPSPEED_EXCLUSION_FILTER]\n",
            "                                            [--deepspeed_inclusion_filter DEEPSPEED_INCLUSION_FILTER]\n",
            "                                            [--deepspeed_multinode_launcher DEEPSPEED_MULTINODE_LAUNCHER]\n",
            "                                            [--deepspeed_moe_layer_cls_names DEEPSPEED_MOE_LAYER_CLS_NAMES]\n",
            "                                            [--fsdp_offload_params FSDP_OFFLOAD_PARAMS]\n",
            "                                            [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]\n",
            "                                            [--fsdp_sharding_strategy FSDP_SHARDING_STRATEGY]\n",
            "                                            [--fsdp_auto_wrap_policy FSDP_AUTO_WRAP_POLICY]\n",
            "                                            [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]\n",
            "                                            [--fsdp_backward_prefetch_policy FSDP_BACKWARD_PREFETCH_POLICY]\n",
            "                                            [--fsdp_backward_prefetch FSDP_BACKWARD_PREFETCH]\n",
            "                                            [--fsdp_state_dict_type FSDP_STATE_DICT_TYPE]\n",
            "                                            [--fsdp_forward_prefetch FSDP_FORWARD_PREFETCH]\n",
            "                                            [--fsdp_use_orig_params FSDP_USE_ORIG_PARAMS]\n",
            "                                            [--fsdp_cpu_ram_efficient_loading FSDP_CPU_RAM_EFFICIENT_LOADING]\n",
            "                                            [--fsdp_sync_module_states FSDP_SYNC_MODULE_STATES]\n",
            "                                            [--fsdp_activation_checkpointing FSDP_ACTIVATION_CHECKPOINTING]\n",
            "                                            [--megatron_lm_tp_degree MEGATRON_LM_TP_DEGREE]\n",
            "                                            [--megatron_lm_pp_degree MEGATRON_LM_PP_DEGREE]\n",
            "                                            [--megatron_lm_num_micro_batches MEGATRON_LM_NUM_MICRO_BATCHES]\n",
            "                                            [--megatron_lm_sequence_parallelism MEGATRON_LM_SEQUENCE_PARALLELISM]\n",
            "                                            [--megatron_lm_recompute_activations MEGATRON_LM_RECOMPUTE_ACTIVATIONS]\n",
            "                                            [--megatron_lm_use_distributed_optimizer MEGATRON_LM_USE_DISTRIBUTED_OPTIMIZER]\n",
            "                                            [--megatron_lm_gradient_clipping MEGATRON_LM_GRADIENT_CLIPPING]\n",
            "                                            [--fp8_backend {te,msamp}]\n",
            "                                            [--fp8_use_autocast_during_eval]\n",
            "                                            [--fp8_margin FP8_MARGIN]\n",
            "                                            [--fp8_interval FP8_INTERVAL]\n",
            "                                            [--fp8_format {E4M3,HYBRID}]\n",
            "                                            [--fp8_amax_history_len FP8_AMAX_HISTORY_LEN]\n",
            "                                            [--fp8_amax_compute_algo {max,most_recent}]\n",
            "                                            [--fp8_override_linear_precision FP8_OVERRIDE_LINEAR_PRECISION]\n",
            "                                            [--fp8_opt_level {O1,O2}]\n",
            "                                            [--aws_access_key_id AWS_ACCESS_KEY_ID]\n",
            "                                            [--aws_secret_access_key AWS_SECRET_ACCESS_KEY]\n",
            "                                            [--debug]\n",
            "                                            [--mpirun_hostfile MPIRUN_HOSTFILE]\n",
            "                                            [--mpirun_ccl MPIRUN_CCL]\n",
            "                                            training_script ...\n",
            "accelerate <command> [<args>] launch: error: the following arguments are required: training_script, training_script_args\n",
            "2024-09-24:12:01:30,519 INFO     [__main__.py:279] Verbosity set to INFO\n",
            "2024-09-24:12:01:30,519 INFO     [__main__.py:303] Including path: ./\n",
            "2024-09-24:12:01:30,562 INFO     [__init__.py:491] `group` and `group_alias` keys in TaskConfigs are deprecated and will be removed in v0.4.5 of lm_eval. The new `tag` field will be used to allow for a shortcut to a group of tasks one does not wish to aggregate metrics across. `group`s which aggregate across subtasks must be only defined in a separate group config file, which will be the official way to create groups that support cross-task aggregation as in `mmlu`. Please see the v0.4.4 patch notes and our documentation: https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/new_task_guide.md#advanced-group-configs for more information.\n",
            "2024-09-24:12:01:34,987 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
            "2024-09-24:12:01:34,988 INFO     [__main__.py:376] Selected Tasks: ['geese_tasks']\n",
            "2024-09-24:12:01:34,989 INFO     [evaluator.py:161] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-09-24:12:01:34,989 INFO     [evaluator.py:198] Initializing hf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B'}\n",
            "2024-09-24:12:01:35,369 INFO     [huggingface.py:130] Using device 'cuda'\n",
            "2024-09-24:12:01:35,992 INFO     [huggingface.py:366] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
            "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:03<00:00,  1.08it/s]\n",
            "2024-09-24:12:01:45,399 INFO     [evaluator.py:279] Setting fewshot random generator seed to 1234\n",
            "2024-09-24:12:01:45,399 INFO     [evaluator.py:279] Setting fewshot random generator seed to 1234\n",
            "2024-09-24:12:01:45,399 INFO     [evaluator.py:279] Setting fewshot random generator seed to 1234\n",
            "2024-09-24:12:01:45,399 INFO     [evaluator.py:279] Setting fewshot random generator seed to 1234\n",
            "2024-09-24:12:01:45,399 WARNING  [model.py:422] model.chat_template was called with the chat_template set to False or None. Therefore no chat template will be applied. Make sure this is an intended behavior.\n",
            "2024-09-24:12:01:45,400 INFO     [task.py:428] Building contexts for geese_noexp on rank 0...\n",
            "100%|███████████████████████████████████████| 800/800 [00:00<00:00, 1844.44it/s]\n",
            "2024-09-24:12:01:45,922 INFO     [task.py:428] Building contexts for geese_dummy on rank 0...\n",
            "100%|███████████████████████████████████████| 800/800 [00:00<00:00, 1634.78it/s]\n",
            "2024-09-24:12:01:46,496 INFO     [task.py:428] Building contexts for geese_human on rank 0...\n",
            "100%|███████████████████████████████████████| 800/800 [00:00<00:00, 1593.36it/s]\n",
            "2024-09-24:12:01:47,081 INFO     [task.py:428] Building contexts for geese_llama3 on rank 0...\n",
            "100%|███████████████████████████████████████| 800/800 [00:00<00:00, 1595.70it/s]\n",
            "2024-09-24:12:01:47,666 INFO     [evaluator.py:485] Running loglikelihood requests\n",
            "Running loglikelihood requests:   0%|                  | 0/9600 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
            "Running loglikelihood requests: 100%|███████| 9600/9600 [02:38<00:00, 60.40it/s]\n",
            "fatal: not a git repository (or any parent up to mount point /)\n",
            "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
            "2024-09-24:12:04:36,701 INFO     [evaluation_tracker.py:206] Saving results aggregated\n",
            "2024-09-24:12:04:36,704 INFO     [evaluation_tracker.py:287] Saving per-sample results for: geese_dummy\n",
            "2024-09-24:12:04:36,769 INFO     [evaluation_tracker.py:287] Saving per-sample results for: geese_human\n",
            "2024-09-24:12:04:36,835 INFO     [evaluation_tracker.py:287] Saving per-sample results for: geese_llama3\n",
            "2024-09-24:12:04:36,923 INFO     [evaluation_tracker.py:287] Saving per-sample results for: geese_noexp\n",
            "hf (pretrained=meta-llama/Meta-Llama-3-8B), gen_kwargs: (None), limit: 800.0, num_fewshot: None, batch_size: 1\n",
            "|   Tasks    |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
            "|------------|-------|------|-----:|------|---|-----:|---|-----:|\n",
            "|geese_dummy |Yaml   |none  |     0|acc   |↑  |0.4800|±  |0.0177|\n",
            "|geese_human |Yaml   |none  |     0|acc   |↑  |0.5663|±  |0.0175|\n",
            "|geese_llama3|Yaml   |none  |     0|acc   |↑  |0.5450|±  |0.0176|\n",
            "|geese_noexp |Yaml   |none  |     0|acc   |↑  |0.4838|±  |0.0177|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run on GPU\n",
        "#!accelerate launch ----no_python\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "!CUDA_VISIBLE_DEVICES=0 accelerate launch --no_python\n",
        "!lm_eval \\\n",
        "    --model hf \\\n",
        "    --model_args pretrained=meta-llama/Meta-Llama-3-8B \\\n",
        "    --include_path ./ \\\n",
        "    --tasks geese_tasks \\\n",
        "    --limit 800 \\\n",
        "    --output output \\\n",
        "    --log_samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run on CPU\n",
        "!accelerate launch --cpu --no_python\n",
        "!lm_eval \\\n",
        "    --model hf \\\n",
        "    --model_args pretrained=meta-llama/Meta-Llama-3-8B,device_map=cpu \\\n",
        "    --include_path ./ \\\n",
        "    --tasks geese_tasks \\\n",
        "    --limit 800 \\\n",
        "    --output output \\\n",
        "    --log_samples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "[1] A. Zaninello, S. Brenna, B. Magnini, *Textual entailment with natural language explanations: The Italian e-rte-3 dataset* in: CLiC-it, 2023. URL: https://ceur-ws.org/Vol-3596/short21.pdf."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "zAov81vTbL2K"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "46f521b73fd943c081c648fd873ebc0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48763b6233374554ae76035c0483066f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4986a21eb560448fa79f4b25cde48951": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b2d90209ec14230b3d58a74ac9b83bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c5689bc13684db8a22681f41863dddd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1d3a8aa016544a78e8821c8f6199e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f61ed33fad754146bdd2ac9db1ba1c48",
              "IPY_MODEL_bfa0af6aeff344c6845e1080a878e92e",
              "IPY_MODEL_fd1ad9e0367d4004aae853b91c3a7617"
            ],
            "layout": "IPY_MODEL_6b2d90209ec14230b3d58a74ac9b83bf"
          }
        },
        "a73f357065d34d7baf0453ae4a8d75e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aed3acd2f2d74003b44079c333a0698e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfa0af6aeff344c6845e1080a878e92e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c5689bc13684db8a22681f41863dddd",
            "max": 5669,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_48763b6233374554ae76035c0483066f",
            "value": 5669
          }
        },
        "f61ed33fad754146bdd2ac9db1ba1c48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a73f357065d34d7baf0453ae4a8d75e2",
            "placeholder": "​",
            "style": "IPY_MODEL_46f521b73fd943c081c648fd873ebc0a",
            "value": "Downloading builder script: 100%"
          }
        },
        "fd1ad9e0367d4004aae853b91c3a7617": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4986a21eb560448fa79f4b25cde48951",
            "placeholder": "​",
            "style": "IPY_MODEL_aed3acd2f2d74003b44079c333a0698e",
            "value": " 5.67k/5.67k [00:00&lt;00:00, 205kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
