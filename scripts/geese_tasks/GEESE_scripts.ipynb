{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw83KAePAhaS"
      },
      "source": [
        "# The GEESE Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnwsOpjda_YW"
      },
      "source": [
        "In this notebook we will be going through a short tutorial on the steps to perform the GEESE task using the e-rte-3-it dataset [1]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAov81vTbL2K"
      },
      "source": [
        "### Install LM-Eval\n",
        "First, you don't have it already, install the LM-Eval library by uncommenting and running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hiosGzq_qZg",
        "outputId": "6ab73e5e-1f54-417e-a388-07e0d870b132"
      },
      "outputs": [],
      "source": [
        "#!pip install git+https://github.com/EleutherAI/lm-evaluation-harness.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9116"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os, tqdm, torch, gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Generate the explanations for the given labels with M1\n",
        "We will use llama-3-3B-instruct as M1 to generate the explanations for the entailment labels in e-rte-3-it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.23it/s]\n",
            "Map:   0%|          | 0/800 [00:00<?, ? examples/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Map: 100%|██████████| 800/800 [2:39:27<00:00, 11.96s/ examples]  \n",
            "Saving the dataset (1/1 shards): 100%|██████████| 800/800 [00:00<00:00, 173749.13 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generation completed. Dataset saved to ./explained_llama3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os, tqdm, torch, gc\n",
        "import re\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset, load_from_disk\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"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\"  # Replace with your token\n",
        "\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Ensure enough GPU memory is available\n",
        "\n",
        "def step_1_generate_explanation(model_name, dataset_name):\n",
        "    # Use the first GPU if available\n",
        "    device = \"cpu\" #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load the model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    #model = model.to(device)\n",
        "\n",
        "    # Create a pipeline for text generation\n",
        "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, pad_token_id=tokenizer.eos_token_id, device=0)\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = load_dataset(dataset_name, split='test')\n",
        "    model_nick = \"llama3\"\n",
        "    \n",
        "    # Function to generate explanations\n",
        "    def add_generated_explanation(example):\n",
        "        text_t = example['text_t']\n",
        "        text_h = example['text_h']\n",
        "        label_str = example['label']\n",
        "        \n",
        "        def generate_explanation(sentence_1, sentence_2, label, exp_type=\"Explain why.\"):\n",
        "            example_sentences = f\"Sentence 1: {sentence_1.strip()}\\nSentence 2: {sentence_2.strip()}\"\n",
        "            prompt = f\"Your task is to provide an explanation for the label assigned for the entailment relationship between two sentences.\\n{example_sentences}\\nEntailment label: {label}\\n{exp_type}\"\n",
        "\n",
        "            #with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "            explanation = generator(prompt, max_length=410, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95, temperature=0.7)\n",
        "\n",
        "            return explanation[0]['generated_text'].split(f'{exp_type}')[-1]\n",
        "\n",
        "        explanation_model = generate_explanation(sentence_1=text_t, sentence_2=text_h, label=label_str)\n",
        "        example[f'anon_{model_nick}'] = explanation_model.strip()\n",
        "\n",
        "        return example\n",
        "    \n",
        "    # Shuffle (and optionally select a smaller chunk of the dataset)\n",
        "    dataset = dataset.shuffle(123) #dataset.shuffle(123).select(range(20))\n",
        "\n",
        "    # Process the dataset (not in batches for memory control)\n",
        "    updated_dataset = dataset.map(add_generated_explanation, batched=False)\n",
        "\n",
        "    output_dir = f\"./explained_{model_nick}\"\n",
        "    updated_dataset.save_to_disk(output_dir)\n",
        "\n",
        "    print(f\"Generation completed. Dataset saved to {output_dir}\")\n",
        "\n",
        "def main():\n",
        "    step_1_generate_explanation(model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\", dataset_name=\"azaninello/e-RTE-3-it\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The entailment label is UNKNOWN because the two sentences do not have a direct entailment relationship. Sentence 1 states that it is rumored that Metin Kaplan ordered the murder of Ibrahim Sofu, while Sentence 2 states that Ibrahim Sofu was killed by Metin Kaplan. The two sentences present different information, with Sentence 1 providing a rumor or unverified information, and Sentence 2 presenting a factual statement. There is no logical connection between the two sentences that would imply one sentence is a direct consequence of the other. Therefore, the entailment label is UNKNOWN, indicating that the relationship between the two sentences is not clear or cannot be determined. \n",
            "\n",
            "Please note that this explanation is based on the assumption that the entailment label is assigned based on the logical relationship between the two sentences. If the entailment label is assigned based on other criteria, such as the similarity or overlap between the two sentences, a different explanation may be necessary.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 800/800 [00:00<00:00, 4535.20 examples/s]\n",
            "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 46.95ba/s]\n",
            "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload completed. Explanations saved as geese-llama-3-full\n"
          ]
        }
      ],
      "source": [
        "# Check the first example of the locally saved dataset, anonymize the explanations to prevent label-leakage, and save the new dataset to the Hugging Face Hub\n",
        "\n",
        "from datasets import load_from_disk\n",
        "\n",
        "model_nick = \"llama3\"\n",
        "output_dir = f\"./explained_{model_nick}\"\n",
        "\n",
        "# Load the saved dataset\n",
        "loaded_dataset = load_from_disk(output_dir)\n",
        "\n",
        "print(loaded_dataset[0][f'anon_{model_nick}'])  # Display the first example, including the generated/anonimized explanations\n",
        "\n",
        "anon_pattern = r\"(\\bYES\\b|\\bNO\\b|\\bUNKNOWN\\b|\\bentail\\w*\\b|\\bcontradict\\w*\\b|\\bneutral\\w*\\b)\"\n",
        "subst_str = \"XXX\"\n",
        "    \n",
        "def anonymize_explanations(example):\n",
        "            # Add the generated explanation to the original model\n",
        "            example['anon_human'] = re.sub(anon_pattern, subst_str, example['explanation'].strip(), flags=re.IGNORECASE)\n",
        "            example[f'anon_{model_nick}'] = re.sub(anon_pattern, subst_str, example[f'anon_{model_nick}'].strip(), flags=re.IGNORECASE)\n",
        "            return example\n",
        "\n",
        "anon_dataset = loaded_dataset.map(anonymize_explanations, batched=False)\n",
        "\n",
        "public_name = \"geese-llama-3-full\"\n",
        "anon_dataset.push_to_hub(public_name, private=False)\n",
        "print(f\"Upload completed. Explanations saved as {public_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rfUeX6n_wkK"
      },
      "source": [
        "## Create \"new\" evaluation task with lm_eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYFUhhfOSJKe"
      },
      "source": [
        "We will configure the prediction tasks of the M2 model as LM-EVAL tasks with YAML configs. With configs, you can fill preset fields to easily set up a task.\n",
        "\n",
        "Here, we write a demo YAML config for a multiple-choice evaluation of e-rte-3-it to get the baseline results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "fthNg3ywO-kA"
      },
      "outputs": [],
      "source": [
        "YAML_geese_noexp_string = '''\n",
        "tag: geese_tasks\n",
        "task: geese_noexp\n",
        "dataset_path: azaninello/geese-llama-3-full\n",
        "dataset_name: default\n",
        "output_type: multiple_choice\n",
        "test_split: test\n",
        "validation_split: test\n",
        "doc_to_text: \"Sentence 1:{{text_t}}\\nSentence 2: {{text_h}}\\nHint: Not given.\\nEntailment label:\"\n",
        "doc_to_target: label\n",
        "doc_to_choice: [\"YES\", \"NO\", \"UNKNOWN\"]\n",
        "metric_list:\n",
        "  - metric: acc\n",
        "    aggregation: mean\n",
        "    higher_is_better: true\n",
        "'''\n",
        "with open('no-exp.yaml', 'w') as f:\n",
        "    f.write(YAML_geese_noexp_string)\n",
        "\n",
        "YAML_geese_dummy_string = '''\n",
        "tag: geese_tasks\n",
        "task: geese_dummy\n",
        "dataset_path: azaninello/geese-llama-3-full\n",
        "dataset_name: default\n",
        "output_type: multiple_choice\n",
        "test_split: test\n",
        "validation_split: test\n",
        "doc_to_text: \"Sentence 1:{{text_t}}\\nSentence 2: {{text_h}}\\nHint: {{text_h}}.\\nEntailment label:\"\n",
        "doc_to_target: label\n",
        "doc_to_choice: [\"YES\", \"NO\", \"UNKNOWN\"]\n",
        "metric_list:\n",
        "  - metric: acc\n",
        "    aggregation: mean\n",
        "    higher_is_better: true\n",
        "'''\n",
        "with open('dummy-exp.yaml', 'w') as f:\n",
        "    f.write(YAML_geese_dummy_string)\n",
        "\n",
        "YAML_geese_human_string = '''\n",
        "tag: geese_tasks\n",
        "task: geese_human\n",
        "dataset_path: azaninello/geese-llama-3-full\n",
        "dataset_name: default\n",
        "output_type: multiple_choice\n",
        "test_split: test\n",
        "validation_split: test\n",
        "doc_to_text: \"Sentence 1:{{text_t}}\\nSentence 2: {{text_h}}\\nHint: {{anon_human}}.\\nEntailment label:\"\n",
        "doc_to_target: label\n",
        "doc_to_choice: [\"YES\", \"NO\", \"UNKNOWN\"]\n",
        "metric_list:\n",
        "  - metric: acc\n",
        "    aggregation: mean\n",
        "    higher_is_better: true\n",
        "'''\n",
        "with open('human-exp.yaml', 'w') as f:\n",
        "    f.write(YAML_geese_human_string)\n",
        "\n",
        "YAML_geese_llama_string = '''\n",
        "tag: geese_tasks\n",
        "task: geese_llama3\n",
        "dataset_path: azaninello/geese-llama-3-full\n",
        "dataset_name: default\n",
        "output_type: multiple_choice\n",
        "test_split: test\n",
        "validation_split: test\n",
        "doc_to_text: \"Sentence 1:{{text_t}}\\nSentence 2: {{text_h}}\\nHint: {{anon_llama3}}.\\nEntailment label:\"\n",
        "doc_to_target: label\n",
        "doc_to_choice: [\"YES\", \"NO\", \"UNKNOWN\"]\n",
        "metric_list:\n",
        "  - metric: acc\n",
        "    aggregation: mean\n",
        "    higher_is_better: true\n",
        "'''\n",
        "with open('llama3-exp.yaml', 'w') as f:\n",
        "    f.write(YAML_geese_llama_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "XceRKCuuDtbn"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-09-24:04:08:45,190 INFO     [__main__.py:279] Verbosity set to INFO\n",
            "2024-09-24:04:08:45,190 INFO     [__main__.py:303] Including path: ./\n",
            "2024-09-24:04:08:45,219 INFO     [__init__.py:491] `group` and `group_alias` keys in TaskConfigs are deprecated and will be removed in v0.4.5 of lm_eval. The new `tag` field will be used to allow for a shortcut to a group of tasks one does not wish to aggregate metrics across. `group`s which aggregate across subtasks must be only defined in a separate group config file, which will be the official way to create groups that support cross-task aggregation as in `mmlu`. Please see the v0.4.4 patch notes and our documentation: https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/new_task_guide.md#advanced-group-configs for more information.\n",
            "2024-09-24:04:08:49,529 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
            "2024-09-24:04:08:49,530 INFO     [__main__.py:376] Selected Tasks: ['geese_tasks']\n",
            "2024-09-24:04:08:49,531 INFO     [evaluator.py:161] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-09-24:04:08:49,531 INFO     [evaluator.py:198] Initializing hf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B'}\n",
            "2024-09-24:04:08:49,962 INFO     [huggingface.py:130] Using device 'cuda'\n",
            "2024-09-24:04:08:50,765 INFO     [huggingface.py:366] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
            "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:04<00:00,  1.07s/it]\n",
            "2024-09-24:04:09:01,842 INFO     [evaluator.py:279] Setting fewshot random generator seed to 1234\n",
            "2024-09-24:04:09:01,842 INFO     [evaluator.py:279] Setting fewshot random generator seed to 1234\n",
            "2024-09-24:04:09:01,842 INFO     [evaluator.py:279] Setting fewshot random generator seed to 1234\n",
            "2024-09-24:04:09:01,842 INFO     [evaluator.py:279] Setting fewshot random generator seed to 1234\n",
            "2024-09-24:04:09:01,842 WARNING  [model.py:422] model.chat_template was called with the chat_template set to False or None. Therefore no chat template will be applied. Make sure this is an intended behavior.\n",
            "2024-09-24:04:09:01,843 INFO     [task.py:428] Building contexts for geese_noexp on rank 0...\n",
            "100%|███████████████████████████████████████| 800/800 [00:00<00:00, 1860.92it/s]\n",
            "2024-09-24:04:09:02,357 INFO     [task.py:428] Building contexts for geese_human on rank 0...\n",
            "100%|███████████████████████████████████████| 800/800 [00:00<00:00, 1564.85it/s]\n",
            "2024-09-24:04:09:02,952 INFO     [task.py:428] Building contexts for geese_llama3 on rank 0...\n",
            "100%|███████████████████████████████████████| 800/800 [00:00<00:00, 1594.14it/s]\n",
            "2024-09-24:04:09:03,540 INFO     [task.py:428] Building contexts for geese_dummy on rank 0...\n",
            "100%|███████████████████████████████████████| 800/800 [00:00<00:00, 1618.14it/s]\n",
            "2024-09-24:04:09:04,120 INFO     [evaluator.py:485] Running loglikelihood requests\n",
            "Running loglikelihood requests:   0%|                  | 0/9600 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
            "Running loglikelihood requests: 100%|███████| 9600/9600 [02:38<00:00, 60.43it/s]\n",
            "2024-09-24:04:11:52,330 INFO     [evaluation_tracker.py:206] Saving results aggregated\n",
            "2024-09-24:04:11:52,332 INFO     [evaluation_tracker.py:287] Saving per-sample results for: geese_dummy\n",
            "2024-09-24:04:11:52,399 INFO     [evaluation_tracker.py:287] Saving per-sample results for: geese_human\n",
            "2024-09-24:04:11:52,467 INFO     [evaluation_tracker.py:287] Saving per-sample results for: geese_llama3\n",
            "2024-09-24:04:11:52,557 INFO     [evaluation_tracker.py:287] Saving per-sample results for: geese_noexp\n",
            "hf (pretrained=meta-llama/Meta-Llama-3-8B), gen_kwargs: (None), limit: 800.0, num_fewshot: None, batch_size: 1\n",
            "|   Tasks    |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
            "|------------|-------|------|-----:|------|---|-----:|---|-----:|\n",
            "|geese_dummy |Yaml   |none  |     0|acc   |↑  |0.4738|±  |0.0177|\n",
            "|geese_human |Yaml   |none  |     0|acc   |↑  |0.5787|±  |0.0175|\n",
            "|geese_llama3|Yaml   |none  |     0|acc   |↑  |0.5425|±  |0.0176|\n",
            "|geese_noexp |Yaml   |none  |     0|acc   |↑  |0.4763|±  |0.0177|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# !accelerate launch --no_python\n",
        "!lm_eval \\\n",
        "    --model hf \\\n",
        "    --model_args pretrained=meta-llama/Meta-Llama-3-8B \\\n",
        "    --include_path ./ \\\n",
        "    --tasks geese_tasks \\\n",
        "    --limit 800 \\\n",
        "    --output output \\\n",
        "    --log_samples\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "zAov81vTbL2K"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "46f521b73fd943c081c648fd873ebc0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48763b6233374554ae76035c0483066f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4986a21eb560448fa79f4b25cde48951": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b2d90209ec14230b3d58a74ac9b83bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c5689bc13684db8a22681f41863dddd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1d3a8aa016544a78e8821c8f6199e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f61ed33fad754146bdd2ac9db1ba1c48",
              "IPY_MODEL_bfa0af6aeff344c6845e1080a878e92e",
              "IPY_MODEL_fd1ad9e0367d4004aae853b91c3a7617"
            ],
            "layout": "IPY_MODEL_6b2d90209ec14230b3d58a74ac9b83bf"
          }
        },
        "a73f357065d34d7baf0453ae4a8d75e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aed3acd2f2d74003b44079c333a0698e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfa0af6aeff344c6845e1080a878e92e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c5689bc13684db8a22681f41863dddd",
            "max": 5669,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_48763b6233374554ae76035c0483066f",
            "value": 5669
          }
        },
        "f61ed33fad754146bdd2ac9db1ba1c48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a73f357065d34d7baf0453ae4a8d75e2",
            "placeholder": "​",
            "style": "IPY_MODEL_46f521b73fd943c081c648fd873ebc0a",
            "value": "Downloading builder script: 100%"
          }
        },
        "fd1ad9e0367d4004aae853b91c3a7617": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4986a21eb560448fa79f4b25cde48951",
            "placeholder": "​",
            "style": "IPY_MODEL_aed3acd2f2d74003b44079c333a0698e",
            "value": " 5.67k/5.67k [00:00&lt;00:00, 205kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
