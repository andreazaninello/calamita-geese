{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw83KAePAhaS"
      },
      "source": [
        "## The GEESE Challenge: running the experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnwsOpjda_YW"
      },
      "source": [
        "In this notebook we will be going through the steps involved in the GEESE task using the e-rte-3-it dataset [1] as our data source."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAov81vTbL2K"
      },
      "source": [
        "### Install Miniconda and LM-Eval\n",
        "(First: we suggest to creat a [Miniconda](https://docs.anaconda.com/miniconda/) environment and work within in.)\n",
        "\n",
        "If you don't have it already, install the LM-Eval library by running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hiosGzq_qZg",
        "outputId": "6ab73e5e-1f54-417e-a388-07e0d870b132"
      },
      "outputs": [],
      "source": [
        "%pip install git+https://github.com/EleutherAI/lm-evaluation-harness.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Generate the explanations for the given labels with M1\n",
        "We will use llama-3-3B-instruct as M1 to generate the explanations for the entailment labels in e-rte-3-it.\n",
        "\n",
        "**IMPORTANT NOTE**\n",
        "\n",
        "This step is implemented within the lm_eval framework. We provide custom code to generate explanations with llama-3, you can change model, prompt and explanation types (in the utils.py script), and generation parameters as you wish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create YAML configurations for each explanation type\n",
        "\n",
        "EXPLANATION_TYPES = [\"whyexp\", \"whynot\", \"implicit\"]\n",
        "\n",
        "YAML_geese_gen_template = '''\n",
        "task: geese_generation_template\n",
        "dataset_path: azaninello/e-RTE-3-it # the name of the dataset on the HF Hub.\n",
        "dataset_name: null # the dataset configuration to use. Leave `null` if your dataset does not require a config to be passed. See https://huggingface.co/docs/datasets/load_hub#configurations for more info.\n",
        "dataset_kwargs: null # any extra keyword arguments that should be passed to the dataset constructor, e.g. `data_dir`.\n",
        "training_split: null\n",
        "validation_split: validation\n",
        "test_split: test\n",
        "fewshot_split: validation\n",
        "output_type: generate_until\n",
        "doc_to_text: !function utils.generate_whyexp_explanation\n",
        "doc_to_target: \"{{explanation}}\"\n",
        "metric_list:\n",
        "  - metric: exact_match\n",
        "    aggregation: mean\n",
        "    higher_is_better: true\n",
        "    ignore_case: true\n",
        "    ignore_punctuation: false\n",
        "generation_kwargs:\n",
        "  until:\n",
        "    - \"</s>\"\n",
        "    - \"<|eot_id|>\"\n",
        "  max_gen_toks: 128\n",
        "  do_sample: false\n",
        "  temperature: 0\n",
        "repeats: 1\n",
        "metadata:\n",
        "  version: 1.3\n",
        "'''\n",
        "with open('geese_generation_template.yaml', 'w') as f:\n",
        "        f.write(YAML_geese_gen_template)\n",
        "\n",
        "for exp_type in EXPLANATION_TYPES:\n",
        "    task = f\"geese_generation_{exp_type}\"\n",
        "    YAML_geese_gen = f'''\n",
        "    include: geese_generation_template.yaml\n",
        "    tag: geese_generation_tasks\n",
        "    task: {task}\n",
        "    doc_to_text: !function ./utils.generate_{exp_type}_explanation\n",
        "    '''\n",
        "\n",
        "    print(YAML_geese_gen)\n",
        "\n",
        "    with open(f'generation-{task}.yaml', 'w') as f:\n",
        "        f.write(YAML_geese_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Step 1 with lm_eval\n",
        "\n",
        "!CUDA_VISIBLE_DEVICES=0,1\n",
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=meta-llama/Meta-Llama-3-8B-Instruct \\\n",
        "    --tasks geese_generation_tasks \\\n",
        "    --batch_size 24 \\\n",
        "    --include_path ./ \\\n",
        "    --output_path generation_outputs_test \\\n",
        "    --limit 48 \\\n",
        "    --log_samples \\\n",
        "    --write_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Post-process explanations\n",
        "After saving your explanations into a dataset, they must be post-processed to prevent label leakage. This step is mandatory. \n",
        "\n",
        "Save the anonimized dataset with you new explanations and push it to the HF hub.\n",
        "\n",
        "(Tip: label your anonymized explanations with a memorable name: you will use them in the next evaluation step. )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import Dataset, load_dataset\n",
        "import os \n",
        "import re\n",
        "\n",
        "MODEL_NAME=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "DATASET_NAME=\"azaninello/e-RTE-3-it\"\n",
        "EXPLANATION_TYPES=[\"human\", \"whyexp\", \"whynot\", \"implicit\"]\n",
        "GENERATION_DIR=os.path.join(\"generation_outputs\", MODEL_NAME.replace(\"/\", \"__\"))\n",
        "EXPLAINED_DATASET_DIR=\"./explained_data\"\n",
        "#LIMIT = 24\n",
        "\n",
        "dataset = load_dataset(DATASET_NAME, split='test')\n",
        "pattern = fr\"samples_geese_generation_({'|'.join(EXPLANATION_TYPES)})_.*?\\.jsonl\"\n",
        "files = [file for file in os.listdir(GENERATION_DIR) if re.match(pattern, file)]\n",
        "\n",
        "def anonymize(text):\n",
        "    if text is None:\n",
        "        return None  # or return \"\" if you prefer an empty string\n",
        "    # the anon_pattern needs to be customized according to task, labels, and language\n",
        "    anon_pattern = r\"(\\bYES\\b|\\bNO\\b|\\bUNKNOWN\\b|\\bentail\\w*\\b|\\bcontradict\\w*\\b|\\bneutr\\w*\\b|\\bimpl\\w*\\b|\\bcontradd\\w*\\b)\"\n",
        "    subst_str = \"XXX\"\n",
        "    text = re.sub(anon_pattern, subst_str, text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "for file in os.listdir(GENERATION_DIR):\n",
        "    for exp_type in EXPLANATION_TYPES:\n",
        "        if f\"geese_generation_{exp_type}_\" in file:\n",
        "            explained_dataset = load_dataset('json', data_files=os.path.join(GENERATION_DIR, file), split='train')\n",
        "            explanations = {item['doc']['id']: item['resps'][0][0].strip() for item in explained_dataset}\n",
        "            dataset = dataset.map(lambda x: {f\"{exp_type}\": explanations.get(x[\"id\"], '')})\n",
        "            dataset = dataset.map(lambda x: {f\"anon_{exp_type}\": anonymize(x[f'{exp_type}']).strip()})\n",
        "\n",
        "# Add new column by mapping the dictionary\n",
        "dataset = dataset.map(lambda x: {f\"anon_human\": anonymize(x['explanation']).strip()})\n",
        "#dataset = dataset.map(lambda x: {\"new_column\": new_features.get(x[\"id\"], None)})\n",
        "\n",
        "# Check the result\n",
        "\n",
        "dataset.save_to_disk(EXPLAINED_DATASET_DIR)\n",
        "print(dataset[0])\n",
        "public_name = \"explained-full-llama-3\"\n",
        "dataset.push_to_hub(public_name, private=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset[700]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rfUeX6n_wkK"
      },
      "source": [
        "### Step 3: Predict labels with explanations with M2\n",
        "\n",
        "We configure the prediction tasks of the M2 model as LM-EVAL tasks with YAML configs. With configs, you can fill preset fields to easily set up a task. Here, we write 4 different YAML config files under the *geese_tasks* tag for multiple-choice evaluation on the e-rte-3-it labels (read from our newly generated dataset). \n",
        "\n",
        "Here we define four configurations corresponding to four explanation sets given as \"Hint\": \n",
        "1. no explanation (Hint = \"Not given.\")\n",
        "2. dummy explanation (Hint = text_h)\n",
        "3. the anonimized human explanations (Hint = anon_human)\n",
        "4. the explanations generated in Step 1 (in our case Hint = anon_whyexp)\n",
        "\n",
        "In the gist configuration file presented at the CALAMITA Challenge, we currently report the **geese_human** configuration, but you are encouraged to propose your own explanations **after post-processing** (see above) before using them, and to read the anonimized explanations from the dataset produced in Step 2.\n",
        "\n",
        "In this step you can can only change the model to use as M2 and recall your own configuration(s) through the task (see above).\n",
        "\n",
        "**IMPORTANT NOTE**\n",
        "\n",
        "This step is implemented within lm_eval for the geese_human configuration described in the gist [configuration file](https://gist.github.com/andreazaninello/6d92daeaa1c264477b32fd22acfbd818). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fthNg3ywO-kA"
      },
      "outputs": [],
      "source": [
        "YAML_geese_noexp_string = '''\n",
        "tag: geese_prediction_baseline\n",
        "task: geese_prediction_noexp\n",
        "dataset_path: azaninello/explained-full-llama-3\n",
        "dataset_name: default\n",
        "output_type: multiple_choice\n",
        "test_split: test\n",
        "validation_split: test\n",
        "doc_to_text: \"Your task is to predict the entailment label between two sentences, selecting one label among YES (entailment), NO (contradiction), or UNKNOWN (neutrality).\\nSentence 1:{{text_t}}\\nSentence 2: {{text_h}}\\nHint: Not given.\\nEntailment label:\"\n",
        "doc_to_target: label\n",
        "doc_to_choice: [\"YES\", \"NO\", \"UNKNOWN\"]\n",
        "metric_list:\n",
        "  - metric: acc\n",
        "    aggregation: mean\n",
        "    higher_is_better: true\n",
        "'''\n",
        "with open('prediction-no-exp.yaml', 'w') as f:\n",
        "    f.write(YAML_geese_noexp_string)\n",
        "\n",
        "YAML_geese_dummy_string = '''\n",
        "include: prediction-no-exp.yaml\n",
        "task: geese_prediction_dummy\n",
        "doc_to_text: \"Your task is to predict the entailment label between two sentences, selecting one label among YES (entailment), NO (contradiction), or UNKNOWN (neutrality).\\nSentence 1:{{text_t}}\\nSentence 2: {{text_h}}\\nHint: {{text_h}}.\\nEntailment label:\"\n",
        "'''\n",
        "with open('prediction-dummy-exp.yaml', 'w') as f:\n",
        "    f.write(YAML_geese_dummy_string)\n",
        "\n",
        "YAML_geese_human_string = '''\n",
        "include: prediction-no-exp.yaml\n",
        "task: geese_prediction_human\n",
        "doc_to_text: \"Your task is to predict the entailment label between two sentences, selecting one label among YES (entailment), NO (contradiction), or UNKNOWN (neutrality).\\nSentence 1:{{text_t}}\\nSentence 2: {{text_h}}\\nHint: {{anon_human}}.\\nEntailment label:\"\n",
        "'''\n",
        "with open('prediction-human-exp.yaml', 'w') as f:\n",
        "    f.write(YAML_geese_human_string)\n",
        "\n",
        "YAML_geese_whyexp_string = '''\n",
        "include: prediction-no-exp.yaml\n",
        "task: geese_prediction_whyexp\n",
        "doc_to_text: \"Your task is to predict the entailment label between two sentences, selecting one label among YES (entailment), NO (contradiction), or UNKNOWN (neutrality).\\nSentence 1:{{text_t}}\\nSentence 2: {{text_h}}\\nHint: {{anon_whyexp}}.\\nEntailment label:\"\n",
        "'''\n",
        "with open('prediction-whyexp-pred.yaml', 'w') as f:\n",
        "    f.write(YAML_geese_whyexp_string)\n",
        "\n",
        "YAML_geese_whynot_string = '''\n",
        "include: prediction-no-exp.yaml\n",
        "tag: geese_prediction_tasks\n",
        "task: geese_prediction_whynot\n",
        "doc_to_text: \"Your task is to predict the entailment label between two sentences, selecting one label among YES (entailment), NO (contradiction), or UNKNOWN (neutrality).\\nSentence 1:{{text_t}}\\nSentence 2: {{text_h}}\\nHint: {{anon_whynot}}.\\nEntailment label:\"\n",
        "'''\n",
        "with open('prediction-whynot-exp.yaml', 'w') as f:\n",
        "    f.write(YAML_geese_whynot_string)\n",
        "\n",
        "YAML_geese_implicit_string = '''\n",
        "include: prediction-no-exp.yaml\n",
        "tag: geese_prediction_tasks\n",
        "task: geese_prediction_implicit\n",
        "doc_to_text: \"Your task is to predict the entailment label between two sentences, selecting one label among YES (entailment), NO (contradiction), or UNKNOWN (neutrality).\\nSentence 1:{{text_t}}\\nSentence 2: {{text_h}}\\nHint: {{anon_implicit}}.\\nEntailment label:\"\n",
        "'''\n",
        "with open('prediction-implicit-exp.yaml', 'w') as f:\n",
        "    f.write(YAML_geese_implicit_string)\n",
        "\n",
        "YAML_geese_obvious_string = '''\n",
        "include: prediction-no-exp.yaml\n",
        "task: geese_prediction_obvious\n",
        "doc_to_text: \"Your task is to predict the entailment label between two sentences, selecting one label among YES (entailment), NO (contradiction), or UNKNOWN (neutrality).\\nSentence 1:{{text_t}}\\nSentence 2: {{text_h}}\\nHint: {{label}}.\\nEntailment label:\"\n",
        "'''\n",
        "with open('prediction-obvious-exp.yaml', 'w') as f:\n",
        "    f.write(YAML_geese_obvious_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XceRKCuuDtbn"
      },
      "outputs": [],
      "source": [
        "# Run on GPU\n",
        "\n",
        "!CUDA_VISIBLE_DEVICES=0,1\n",
        "!lm_eval \\\n",
        "    --model hf \\\n",
        "    --model_args pretrained=meta-llama/Meta-Llama-3-8B \\\n",
        "    --include_path ./ \\\n",
        "    --tasks geese_prediction_baseline \\\n",
        "    --batch_size 24 \\\n",
        "    --output prediction_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "[1] A. Zaninello, S. Brenna, B. Magnini, *Textual entailment with natural language explanations: The Italian e-rte-3 dataset* in: CLiC-it, 2023. URL: https://ceur-ws.org/Vol-3596/short21.pdf."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "zAov81vTbL2K"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "46f521b73fd943c081c648fd873ebc0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48763b6233374554ae76035c0483066f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4986a21eb560448fa79f4b25cde48951": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b2d90209ec14230b3d58a74ac9b83bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c5689bc13684db8a22681f41863dddd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1d3a8aa016544a78e8821c8f6199e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f61ed33fad754146bdd2ac9db1ba1c48",
              "IPY_MODEL_bfa0af6aeff344c6845e1080a878e92e",
              "IPY_MODEL_fd1ad9e0367d4004aae853b91c3a7617"
            ],
            "layout": "IPY_MODEL_6b2d90209ec14230b3d58a74ac9b83bf"
          }
        },
        "a73f357065d34d7baf0453ae4a8d75e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aed3acd2f2d74003b44079c333a0698e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfa0af6aeff344c6845e1080a878e92e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c5689bc13684db8a22681f41863dddd",
            "max": 5669,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_48763b6233374554ae76035c0483066f",
            "value": 5669
          }
        },
        "f61ed33fad754146bdd2ac9db1ba1c48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a73f357065d34d7baf0453ae4a8d75e2",
            "placeholder": "​",
            "style": "IPY_MODEL_46f521b73fd943c081c648fd873ebc0a",
            "value": "Downloading builder script: 100%"
          }
        },
        "fd1ad9e0367d4004aae853b91c3a7617": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4986a21eb560448fa79f4b25cde48951",
            "placeholder": "​",
            "style": "IPY_MODEL_aed3acd2f2d74003b44079c333a0698e",
            "value": " 5.67k/5.67k [00:00&lt;00:00, 205kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
